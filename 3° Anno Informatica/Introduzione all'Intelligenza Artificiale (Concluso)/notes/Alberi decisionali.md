## ID3
Algoritmo che, dato un training set di esempi costruisce un decision tree eseguendo una ricerca nello spazio dei decision tree. La costruzione è **top-down** ed è di tipo **greedy**.
 
```
# X: esempi di training
# T: attributo target
# Attrs: altri attributi, inizialmente tutti gli attributi
def ID3(X, T, Attrs):
	Create Root Node
	if all X are +: 
		return Root with class +
	if all X are -: 
		return Root with class -
	if Attrs is empty: 
		return Root with class most comon value of T in X
	else
		A <- best attribute; decision attribute for Root <- A
		for each possible value v_i of A:
			add a new branch bellow Root, for test A = v_i
			X_i <- subset of X con A = v_i
			if X_i is empty:
				add a new leaf with class the most common value of T in X
			else:
				add the subtree generated by ID3(X_i, T, Attr - {A})
	return Root
```

##### Decisione best attribute
Usiamo la notazione di entropia, che misura l'impurità di una collezione di esempi:
$$Entropy(S) = -p_+ \log_2(p_+) - p_- \log_2 (p_-)$$
Con $p_+$ proporzione esempi positivi in S, mentre $p_+$ proporzione esempi negativi in S.

Da qui il il miglior attributo è quello che massimizza la $Gain$ funzione.
$$Gain(S, A) = Entropy(S) - \sum_{v \in Values(A)} \frac{S_v}{S}Entropy(S_v)$$
Dove $values(A)$ sono i possibili valori per A, $S_v$ è un sottoinsieme di esempi di $S$ per i quali $A$ ha valore $v$. 

Per evitare che avvenga una frammentazione in troppi piccoli sottoinsiemi $S_v$ introduciamo
$$GainRation(S, A) = \frac{Gain(S, A)}{SplitInInformation(S, A)}$$
Dove abbiamo
$$SplitInInformation(S, A) = -\sum_{i=1}^C \frac{|S_i|}{|S|} \log_2\frac{|S_i|}{|S|}$$

- **Search bias**: ordine di ricerca impostato nel LSM (nel caso dei modelli lineari)
- **Language bias**: dovuta alla scelta della funzione lineare (non permette di risolvere problemi non lineari)

##### Reduced error pruning
Si rimuove un sotto albero di un determinato nodo, questo nodo diventa a sua volta una foglia con il valore più comune nel sotto albero. 

Questa operazione viene fatta se l'eliminazione del sotto-albero non peggiora le prestazioni. I nodi vengono potati iterativamente
##### Regole post potatura
Sono un set di regole che vengono applicate nel momento in cui finisce l'operazione di potatura.
##### Attributi a valori continui
Spesso ci si trova ad utilizzare valori che non sono True o False, ma possono essere rappresentati in spazi continui, in questo caso si stabiliscono delle soglie nel quale quel valore prende valore True e False.