% !TeX spellcheck = it_IT
\newpage
\section{Metodi iterativi}
Una classe di metodi che permette di risolvere sistemi lineari è quella dei metodi iterativi. L'idea è di costruire una successione di vettori partendo dalla matrice che convergano alla soluzione del sistema lineare.
\begin{equation*}
	\{X_k\}_{k\in \mathbb{N}}
\end{equation*}
\begin{observation}
	Non potendo generare infiniti termini, ad un certo punto ci sarà bisogno di fermarsi quando si pensa di essere sufficientemente vicini alla soluzione.
\end{observation}
Diciamo che
\begin{equation}
	\lim_{k\to +\infty}x_k \Leftrightarrow \lim_{k\to +\infty} \lvert\lvert x_k - x\rvert\rvert_{\infty} = 0
\end{equation}
\noindent Questo è vero perché:
\begin{equation*}
	\forall j = 1, \ldots, n \quad 0 \leq \lvert x^{(k)}_j - x_j \rvert \leq \lvert\lvert x^{(k)} - x\rvert\rvert_{\infty}
\end{equation*}

Abbiamo una matrice invertibile $A$. Supponiamo di scomporre la matrice come:
\begin{equation}
	A=M-N
\end{equation}
con l'assunzione che anche $M$ sia invertibile ($det(M)=0$). A questo punto possiamo dire che:
\begin{equation*}
	Ax=b \Leftrightarrow (M-N)x = b\Leftrightarrow Mx=Nx+b \Leftrightarrow x=M^{-1}Nx + M{-1}b \Leftrightarrow x=Px+q
\end{equation*}
Dovendo trovare la soluzione di $x=Px+q$, possiamo scegliere un vettore iniziale $x_0 \in \mathbb{R}^n$ e costruirci la successione di vettori
\begin{equation}
	\label{equation:succ}
	x_{k+1}=Px_k + q
\end{equation}
Se questa successione converge ho trovato la soluzione del sistema lineare.
\begin{observation}
	Nell'implementazione pratica non userò mai l'equazione \ref{equation:succ} ma invece
	\begin{equation}
		Mx_{k+1}=Nx_k + b
	\end{equation}
\end{observation}

\subsection{Convergenza}
\begin{example}
	Prendiamo
	\begin{equation*}
		\begin{bmatrix}
			2 & 1 \\
			1 & 2
		\end{bmatrix} \quad
		b = \begin{bmatrix}
			0 \\ 0
		\end{bmatrix} \quad
		Ax=b \Leftrightarrow x=\begin{bmatrix}
			0 \\ 0
		\end{bmatrix}
	\end{equation*}
	Partiamo definendo le due matrici per la scomposizione:
	\begin{equation*}
		M=\begin{bmatrix}
			2 & 0 \\ 0 & 2
		\end{bmatrix} \quad
		N = \begin{bmatrix}
			0 & -1 \\ -1 & 0
		\end{bmatrix}
	\end{equation*}
	e calcolando la matrice $P$
	\begin{equation*}
		P = M^{-1}N = \begin{bmatrix}
			\frac{1}{2} & 0 \\ 0 & \frac{1}{2}
		\end{bmatrix}\begin{bmatrix}
		0 & -1 \\ -1 & 0
		\end{bmatrix} = \begin{bmatrix}
		0 & -\frac{1}{2} \\ -\frac{1}{2} & 0
		\end{bmatrix}
	\end{equation*}
	e il vettore $q$
	%TODO
	Iniziamo il metodo iterativo:
	\begin{equation*}
		\begin{split}
			& x^{k+1} = Px^{(k)} = \begin{bmatrix}
				0 & -\frac{1}{2} \\ -\frac{1}{2} & 0
			\end{bmatrix} x^{(k)} \\
			& x^{(1)} =  \begin{bmatrix}
				0 & -\frac{1}{2} \\ -\frac{1}{2} & 0
			\end{bmatrix} \begin{bmatrix}
			x \\ y
			\end{bmatrix} = \begin{bmatrix}
			-\frac{1}{2} y \\ -\frac{1}{2} x
			\end{bmatrix} \\
			& x^{(2)} =  \begin{bmatrix}
				0 & -\frac{1}{2} \\ -\frac{1}{2} & 0
			\end{bmatrix} \begin{bmatrix}
				-\frac{1}{2}y \\ -\frac{1}{2}x
			\end{bmatrix} = \begin{bmatrix}
				\frac{1}{4} x \\ \frac{1}{4} y
			\end{bmatrix}
		\end{split}
	\end{equation*}
	e notiamo che la successione tende a 
	\begin{equation*}
		\begin{bmatrix}
			0 \\ 0
		\end{bmatrix}
	\end{equation*}
	Se prendiamo invece
	\begin{equation*}
		M=\begin{bmatrix}
			0 & 1 \\ 1 & 0
		\end{bmatrix} \quad
		N = \begin{bmatrix}
			-2 & 0 \\ 0 & -2
		\end{bmatrix}
	\end{equation*} con
	\begin{equation*}
		P = M^{-1}N = \begin{bmatrix}
			0 & 1 \\ 1 &0
		\end{bmatrix}\begin{bmatrix}
			-2 & 0 \\ 0 & -2
		\end{bmatrix} = \begin{bmatrix}
			0 &-2 & -2 & 0
		\end{bmatrix}
	\end{equation*}
	abbiamo che la successione \textbf{diverge}.
\end{example}
\begin{definition}
	Un metodo iterativo è convergente se
	\begin{equation}
		\forall x^{(0)} \in \mathbb{R}^n \quad x_k \to x
	\end{equation}
	Ovvero se per ogni vettore di partenza scelto, il metodo converge.
\end{definition}

\begin{theorem}
	Dato $x^{(k+1)} = Px^{(k)}+q$, se
	\begin{equation}
		\exists \lvert\lvert\cdot\rvert\rvert \text{ tc } \lvert\lvert P \rvert\rvert <1
	\end{equation}
	allora il metodo è convergente.
\end{theorem}

\begin{example}
	Data una matrice
	\begin{equation*}
		A = \begin{bmatrix}
			3 & -1 & \\ 
			-1 & \ddots & \ddots \\
			 & \ddots & \ddots & -1\\
			 & & -1 & 3
		\end{bmatrix}
	\end{equation*}
	definiamo le matrici per la scomposizione
	\begin{equation*}
		M = \begin{bmatrix}
			3 \\
			& \ddots \\
			& & 3
		\end{bmatrix} \quad
		N = \begin{bmatrix}
			0 & 1 \\
			1 & \ddots & \ddots \\
			& \ddots & \ddots & 1 \\
			& & 1 &0
		\end{bmatrix}
	\end{equation*}
	Per capire se il metodo è convergente dobbiamo calcolare la norma infinito di $P$:
	\begin{equation*}
		M^{-1} N = \begin{bmatrix}
			0 & \frac{1}{3} \\
			\frac{1}{3} & \ddots & \ddots \\
			& \ddots & \ddots & \frac{1}{3}\\
			& & \frac{1}{3} & 0
		\end{bmatrix} \quad
		\lvert\lvert P \rvert\rvert_{\infty}= \frac{1}{3} + \frac{1}{3} = \frac{2}{3} < 1
	\end{equation*}
	Il problema di questo metodo iterativo è che ha complessità $O(n)$ per ogni iterazione e non è quindi competitivo con l'eliminazione Gaussiana.
\end{example}

\begin{example}
	Data una matrice
	\begin{equation*}
		A = \begin{bmatrix}
			T & I & \\ 
			-I & \ddots & \ddots \\
			& \ddots & \ddots & I\\
			& & -I & T
		\end{bmatrix} \quad
		T = \begin{bmatrix}
			5 & -1 & \\ 
			-1 & \ddots & \ddots \\
			& \ddots & \ddots & -1\\
			& & -1 & 5
		\end{bmatrix}
	\end{equation*}
	In questo caso i metodi iterativi sono vantaggiosi poiché anche se la matrice è predominante diagonale (e quindi permette Gauss senza problemi), a causa dell'effetto del fill-in lo rende sconveniente.
\end{example}

\begin{example}
	Data una matrice
	\begin{equation*}
		A = \begin{bmatrix}
			n & -1 & \ldots & -1\\ 
			-1 & \ddots & \ddots & \vdots \\
			\vdots & \ddots & \ddots & -1\\
			-1 & \ldots & -1 & n
		\end{bmatrix}
	\end{equation*}
	calcoliamo $P$
	\begin{equation*}
		N = \begin{bmatrix}
			0 & 1 & \ldots & 1\\ 
			1 & \ddots & \ddots & \vdots \\
			\vdots & \ddots & \ddots & 1\\
			1 & \ldots & 1 & 0
		\end{bmatrix} \quad
		P = \begin{bmatrix}
			0 & \frac{1}{n} & \ldots & \frac{1}{n}\\ 
			\frac{1}{n} & \ddots & \ddots & \vdots \\
			\vdots & \ddots & \ddots & \frac{1}{n}\\
			\frac{1}{n} & \ldots & \frac{1}{n} & 0
		\end{bmatrix}
	\end{equation*}
	La sua norma vale
	\begin{equation*}
		\lvert\lvert P \rvert\rvert_{\infty} = \frac{n-1}{n}
	\end{equation*} e quindi converge.
\end{example}

\begin{note}
	Se la norma vale $1$ non posso dire nulla sulla convergenza.
\end{note}

\begin{theorem}
	\begin{equation}
		x^{(k+1)} = Px^{(k)}+q \text{ è convergente } \Leftrightarrow \phi(P)<1
	\end{equation}
\end{theorem}

\begin{example}
		Data una matrice
	\begin{equation*}
		A = \begin{bmatrix}
			1 & & & \alpha\\ 
			-1 & \ddots &  & \\
			& \ddots & \ddots &\\
			& & -1 & 1
		\end{bmatrix}
	\end{equation*}
	troviamo le matrici per la scomposizione
	\begin{equation*}
		M=I \quad
		N = \begin{bmatrix}
			0 & &  & -\alpha\\ 
			1& \ddots & &\\
			 & \ddots & \ddots &\\
			&& 1 & 0
		\end{bmatrix}
	\end{equation*}
	Per quali $\alpha$ il metodo è convergente? \\
	Troviamo la fattorizzazione LU della matrice per il calcolo del determinante e otteniamo così il polinomio caratteristico:
	\begin{equation*}
		x^n + \alpha
	\end{equation*}
	Dobbiamo poi trovare il modulo degli autovalori per poterli confrontare:
	\begin{equation*}
		\begin{split}
			x^n=-\alpha \\
			\lvert\lambda\rvert^n = \lvert -\alpha \rvert \Rightarrow \lvert \lambda \rvert = \sqrt[n]{\lvert \alpha \rvert} \\
			\sqrt[n]{\lvert \alpha \rvert} < 1 \Leftrightarrow \lvert \alpha \rvert < 1
		\end{split}
	\end{equation*}
\end{example}

\subsection{Metodo di Jacobi}
In questa tecnica prendiamo $M$ come la matrice diagonale principale:
\begin{equation*}
	M=diag(A) \quad A=\begin{bmatrix}
		a_{11} & \ldots & a_{1n} \\
		\vdots & & \\
		a_{n1} & \ldots & a_{nn}
	\end{bmatrix} \quad
	M = \begin{bmatrix}
		a_{11} \\
		& \ddots \\
		& & a_{nn}
	\end{bmatrix}
\end{equation*}
Chiaramente questo metodo è applicabile sono quando la diagonale non contiene zeri, in quanto altrimenti non sarebbe invertibile.\\
Possiamo ottenere la matrice $N$ facendo $M-A$:
\begin{equation*}
	N=\begin{bmatrix}
		0 & -a_{12} & \ldots & \ldots & -a_{1n} \\
		-a_{21} & \ddots & \ddots && \vdots \\
		\vdots & \ddots & \ddots & \ddots & \vdots \\
		\vdots & & \ddots & \ddots & -a_{n-1 n}\\
		-a_{n1} & & &-a_{n n-1} & 0 
	\end{bmatrix}
\end{equation*}
e il vettore al tempo $k$:
\begin{equation}
	x_e^{(k+1)} = \frac{1}{a_{ll}} \begin{bmatrix}\\
		b_l - \sum_{j=1, j\neq l}^n a_{lj}x_j^{(k)}\\
		\\
	\end{bmatrix} \quad l=1, \ldots, n
\end{equation}
che avrà complessità $O(nnz(A))$ essendo molto parallelizzabile.

\begin{example}
	Data la matrice
	\begin{equation*}
		A=\begin{bmatrix}
			1 & & &-\alpha \\
			& \ddots && \vdots\\
			& & \ddots & -\alpha\\
			-\beta & & & 1
		\end{bmatrix}
	\end{equation*}
	la sua matrice $J$ è
	\begin{equation*}
		J = \begin{bmatrix}
			 & & &\alpha \\
			&  && \vdots\\
			& &  & \alpha\\
			\beta & \ldots& \beta& 0
		\end{bmatrix}
	\end{equation*}
	Utilizziamo la fattorizzazione LU per calcolare il polinomio caratteristico:
	\begin{equation*}
		det(\lambda I - J) = det \begin{bmatrix}
			\lambda & & &-\alpha \\
			&  \ddots&& \vdots\\
			& &  \ddots& -\alpha\\
			-\beta & \ldots& 0& \lambda
		\end{bmatrix} = \begin{bmatrix}
		& & & \\
		&  I&& 0\\
		& &  & \\
		-\frac{\beta}{\lambda}&0 &0  & 1
		\end{bmatrix} \begin{bmatrix}
		\lambda& & & -\alpha\\
		&  \ddots&& \vdots\\
		& &  \lambda& -\alpha\\
		&&  & u
		\end{bmatrix}
	\end{equation*}
	dove $u = \lambda - \frac{\alpha\beta}{\lambda}$ e il raggio spettrale vale
	\begin{equation*}
		\phi(J)< 1 \Leftrightarrow \sqrt{\lvert \alpha \beta \rvert} < 1
	\end{equation*}
	Dimostrare che per i valori per cui Jacobi è convergente, la matrice è invertibile.\\
	Sappiamo che $A$ non è invertibile se e solo se:
	\begin{equation*}
		\exists x \neq 0 \vert Ax=0 \Leftrightarrow (M-N)x=0 \Leftrightarrow Mx=Nx \Leftrightarrow x=M^{-1}Nx \Leftrightarrow x=Px
	\end{equation*}
	Questo ci fa capire che che possiamo scriverlo come $Px=\lambda x$e che il metodo quindi non è convergente in quanto ha autovalore $\lambda$con autovettore $x$. Quindi se è convergente allora è invertibile.
\end{example}

\subsection{Metodo di Gauss-Seidel}
Gauss ragiona sul fatto che ad ogni iterazione avrò calcolato tutte le componenti precedenti fino a $l-1$ al tempo $k+1$. Riscrive quindi l'iterazione di Jacobi nella seguente forma:
\begin{equation}
	x_e^{(k+1)} = \frac{1}{a_{ll}} \begin{bmatrix}\\
		b_e-\sum_{j=1}^{l-1}a_{lj}x_j^{(k+1)}-\sum_{j=l+1}^{n}a_{ej}x_j^{(k)}\\\\
	\end{bmatrix}
\end{equation}
La differenza è che questo metodo, rispetto a quello di Jacobi, perde il parallelismo ma converge più velocemente.

\begin{theorem}
	Se $A$ è predominante diagonale, allora:
	\begin{itemize}
		\item I metodi di Jacobi e Gauss-Seidel sono applicabili
		\item I metodi di Jacobi e Gauss-Seidel sono convergenti
	\end{itemize}
\end{theorem}
\begin{demostration}
	Dimostriamo i due punti del teorema:
	\begin{itemize}
		\item Il primo punto è semplice in quanto se la matrice è predominante diagonale per forza di cose avremo almeno un valore diverso da $0$ (perché vale la disuguaglianza stretta)
		\item Per dimostrare che sono convergenti vogliamo mostrare che:
		\begin{equation*}
			A \text{predominante diagonale }\Rightarrow \phi(J), \phi(GS) <1 \Rightarrow \text{Convergente} 
		\end{equation*}
		Stiamo usando il fatto che il raggio spettrale sia \textbf{sufficiente} a garantire la convergenza.
		\begin{equation*}
			\begin{split}
				det(\lambda I-P)\\ &= det(\lambda I - M^{-1}N) \\&=det(\lambda M^{-1}M - M^{-1}N)\\ & =det(M^{-1}(\lambda M -N)) \\&=det(M^{-1}) \cdot det(\lambda M - N) \\
				det(\lambda I -P ) = 0 \Leftrightarrow det(\lambda M - N) = 0
			\end{split}
		\end{equation*}
		Quindi se $\lambda$ è autovalore di $P$ allora
		\begin{equation}
			\label{equation:absurd_dem_jacobi}
			det(\lambda M -N)=0
		\end{equation}
		Assumiamo che esista un autovalore $\lambda$ di $P$ con $\lvert \lambda \rvert \geq 1$:
		\begin{equation*}
			\lambda M - N = \lambda \begin{bmatrix}
				a_{11}\\
				& \ddots \\
				& & a_{nn}
			\end{bmatrix} - \begin{bmatrix}
			0 & a_{12} & \ldots & . \\
			& \ddots & \ddots & \vdots \\
			& & \ddots & \vdots \\
			& & & 0
			
			\end{bmatrix} = 
			\begin{bmatrix}
				\lambda a_{11} & a_{12} & \ldots & a_{1n} \\
				& \ddots & \ddots  & \vdots \\
				&  & \ddots & \vdots \\
				& & & \lambda a_{nn}
				
			\end{bmatrix}
		\end{equation*}
		dimostriamo ora che la matrice è predominante diagonale
		\begin{equation*}
			\lvert \lambda_{ll} \rvert > \sum_{j=1, j\neq l}^{n} \lvert a_{lj} \rvert \quad \lvert \lambda \lvert \lvert a_{ll} \rvert \geq \lvert a_{ll} \rvert > \sum_{j=1, j \neq l}^{n} \lvert a_{lj} \rvert
		\end{equation*}
		ma se è predominante diagonale allora la matrice è per forza invertibile e il determinante non è 0. Quindi abbiamo dimostrato per assurdo che l'uguaglianza \ref{equation:absurd_dem_jacobi} non è vera.
	\end{itemize}
\end{demostration}

\subsection{Criteri di arresto}
Un criterio tipico di arresto per capire quando fermare le nostre iterazioni è:
\begin{equation}
	\label{equation:tolleranza}
	\lvert \lvert x_{k+1} - x_n \rvert \rvert \leq \text{tolleranza}
\end{equation}
che in codice diventa:
\begin{lstlisting}
	while(err > tol AND iterazoni <= it_max)
		calcola x_new partendo da x_old
		voluto err = ||x_new - x_old ||
		x_old = x_new
		iterazioni = iterazioni +1
	end
\end{lstlisting}
Per capire quando fermarsi possiamo mettere in relazione la differenza al punto \ref{equation:tolleranza} con la soluzione come segue:
\begin{equation*}
	\begin{split}
		x_{k+1}-x_k = \\
		& = x_{k+1}-x+x-x_k \\
		& = Px_k+1-Px-q+x-x_k \\
		& =P(x_k-x)+x-x_k \\
		&= (P-I)(x_k-x)
	\end{split}
\end{equation*}
dove la matrice $P-I$ è invertibile dato che, quando sottraiamo l'identità, gli autovalori sono quelli della prima matrice meno $1$ e quindi per avere autovalori uguali a $0$ dovremmo avere che un autovalore di $P$ è $1$, ma è impossibile perché dato che è convergente abbiamo che il loro valore assoluto è $<1$. Ottengo quindi:
\begin{equation}
	\lvert\lvert x_k - x \rvert\rvert \leq \lvert\lvert (P-I)^{-1}\rvert\rvert \cdot \lvert\lvert x_{k+1}-x_{k}\rvert\rvert
\end{equation}

\begin{example}
	Data la seguente matrice
	\begin{equation*}
		\begin{bmatrix}
			1 & \ldots & \ldots&x\\
			& \ddots & & \vdots \\
			& & \ddots & x\\
			x & \ldots& \ldots& 1
		\end{bmatrix}
	\end{equation*}
	possiamo dire che è predominante diagonale se $\lvert x \rvert < 1$.\\\\
	Per quali valori di $x$ il metodo di Gauss-Seidel converge? Bisogna guardare il raggio spettrale. La matrice di iterazione per il metodo GS sarà fatta:
	\begin{equation*}
		GS=\begin{bmatrix}
			1 & \\
			& \ddots \\
			& & \ddots \\
			x & \ldots & x & 1
		\end{bmatrix}^{-1}\begin{bmatrix}
		& & & & -x \\
		& & & & \vdots \\
		& & &&  -x \\
		& & && 0
		\end{bmatrix} = \begin{bmatrix}
		0 & \ldots & 0 & y_1 \\
		\vdots & & \vdots & \vdots \\
		\vdots & & \vdots & \vdots \\
		0 & \ldots & 0 & y_n
		\end{bmatrix}
	\end{equation*}
	Dobbiamo quindi risolvere:
	\begin{equation*}
		\begin{bmatrix}
			-x \\ \vdots \\ -x \\0
		\end{bmatrix}=My \quad
		\begin{bmatrix}
			1 & \\
			& \ddots \\
			& & \ddots \\
			x & \ldots & x & 1
		\end{bmatrix}\begin{bmatrix}
		y_1 \\ \vdots \\ \vdots \\ y_n
		\end{bmatrix} = 
		\begin{bmatrix}
		-x \\ \vdots \\ -x \\0
		\end{bmatrix}
	\end{equation*}
	\begin{equation*}
		\begin{split}
			& y_n = -x \\
			&y_2=-x \\
			& \ldots \\
			& y_{n+1}=-x \\
			& x\cdot y_1+x\cdot y_2+\ldots + x\cdot y_{n-1}+y_n = 0
		\end{split}
	\end{equation*}
	quindi abbiamo che il raggio spettrale vale
	\begin{equation*}
		\phi(GS) = (n-1)x^2
	\end{equation*}
	che deve essere minore di 1:
	\begin{equation*}
		(n-1)x^2 <1 \Leftrightarrow x^2<\frac{1}{n-1} \Leftrightarrow - \frac{1}{\sqrt{n-1}} < x < \frac{1}{n-1}
	\end{equation*}
\end{example}