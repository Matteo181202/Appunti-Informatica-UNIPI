\section{Formulario}
\subsection{Statistica descrittiva}
\subsubsection{Indici statistici}
\begin{align}
	&\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i
	\tag{Media campionaria}\\
	& var(x)=\frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2
	\tag{Varianza campionaria}\\
	&\sigma(x)=\sqrt{var(x)}
	\tag{Deviazione standard} \\
	& b=\frac{1}{\sigma^3}\cdot \frac{1}{n} \sum_{i=1}^{n}(x_i - \bar{x})^3
	\tag{Sample skewness}
\end{align}
\begin{proposition}
	\begin{equation}
		\frac{\# \{x_i : \lvert x_i - \bar{x} \rvert > d\}}{n-1} \leq \frac{var(x)}{d^2}
	\end{equation}
\end{proposition}

\subsubsection{Quantili}
\begin{align}
	& F_e(t)=\frac{\# \{i \vert x_i \leq t\}}{n} \tag{Funzione di ripartizione empirica}
\end{align}
\subsubsection{Dati multivariati}
\begin{align}
	& cov(x,y)=\sum_{i=1}^{n}\frac{(x_i - \bar{x})(y_i-\bar{y})}{n-1} \tag{Covarianza campionaria} \\
	& r(x,y)=\frac{cov(x,y)}{\sigma(x)\sigma(y)} = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2} \sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}} \tag{Coefficiente di correlazione}
\end{align}

\subsection{Probabilità e indipendenza}
\subsubsection{Spazi di probabilità}
\begin{align}
	& \mathbb{P}\bigg(\bigcup_{n=1}^{+\infty}A_n\bigg) = \sum_{n=1}^{+\infty} \mathbb{P}(A_n) \tag{$\sigma$-addittività}
\end{align}
\begin{proposition}
	Operazioni su spazi di probabilità:
	\begin{itemize}
		\item $\mathbb{P}(A^c) = 1-\mathbb{P}(A)$
		\item $\mathbb{P}(\emptyset) = 0$
		\item $\mathbb{P}(A \setminus B) = \mathbb{P}(A)-\mathbb{P}(B)$
		\item $\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)$
		\item $\mathbb{P}(A \cup B \cup C) = \mathbb{P}(A) + \mathbb{P}(B) + \mathbb{P}(C) - \mathbb{P}(A \cap B) - \mathbb{P}(A \cap C) - \mathbb{P}(B \cap C) + \mathbb{P}(A \cap B \cap C)$
	\end{itemize}
\end{proposition}

\begin{proposition}
	Vale:
	\begin{equation}
		\mathbb{P}(A) = \lim_{n \to \infty} \mathbb{P}(A_n)
	\end{equation}
\end{proposition}

\subsubsection{Probabilità discreta}
\begin{align}
	& \mathbb{P}(A) = \frac{\# A}{\# \Omega} \tag{Probabilità}
\end{align}
\begin{proposition}
	Calcolo combinatorio:
	\begin{itemize}
		\item Sequenze ordinate con possibile ripetizione di $k$ numeri da $1$ a $n$: $n^k$
		\item Numero di modi in cui si può ordinare $\{1, \ldots, n\}$: $n!$
		\item Numero di sequenze ordinate senza ripetizione di $k$ numeri di $\{1, \ldots, n\}$: $\frac{n!}{(n-k)!}$
		\item Numero di sottoinsiemi di $\{1, \ldots, n\}$ formati da $k$ elementi: $\binom{n}{k} = \frac{n!}{k!(n-k)!}$
	\end{itemize}
\end{proposition}

\begin{definition}[Funzione di massa]
	Se $\Omega=\{x_1,x_2,\ldots\} \subset \mathbb{R}$ è un sottoinsieme numerabile:
	\begin{equation}
		\Omega \ni x_i \mapsto p(x_i) = \mathbb{P}(\{x_i\}) \in [0,1]
	\end{equation}
	e valgono:
	\begin{align}
		& \mathbb{P}(A) = \sum_{i : x_i \in A} p(x_i) \quad\quad \forall A \subseteq \mathbb{R} \\
		& p(x_i) \geq 0 \\
		& \sum_{i=1,2,\ldots} p(x_i) = 1
	\end{align}
\end{definition}

\subsubsection{Probabilità condizionata}
\begin{align}
	& \mathbb{P}(A \vert B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} \tag{Probabilità condizionata} \\
	& \mathbb{P}(A_1 \cap \ldots \cap A_n) = \mathbb{P}(A_1) \cdot \mathbb{P}(A_2 \vert A_1) \cdot \ldots \cdot \mathbb{P}(A_n \vert A_1 \cap \ldots \cap A_{n-1}) \tag{Condizionamento ripetuto} \\
	& \mathbb{P}(A) = \sum_{i=1}^{n} \mathbb{P}(A \vert B_i)\mathbb{P}(B_i) \tag{Formula di fattorizzazione} \\
	& \mathbb{P}(B \vert A) = \frac{\mathbb{P}(A \vert B)\mathbb{P}(B)}{\mathbb{P}(A)} \tag{Formula di Bayes} \\
	& \mathbb{P}(B_i \vert A) = \frac{\mathbb{P}(A \vert B_i)\mathbb{P}(B_i)}{\mathbb{P}(A)} = \frac{\mathbb{P}(A \vert B_i) \mathbb{P}(B_i)}{\sum_{j=1}^{n}\mathbb{P}(A \vert B_j)\mathbb{P}(B_j)} \tag{Formula di Bayes - Sistema di alternative} \\
	& \mathbb{P}(A_{i_1} \cap \ldots \cap A_{i_k}) = \mathbb{P}(A_{i_1}) \cdot \ldots \cdot \mathbb{P}(A_{i_k}) \tag{Eventi indipendenti}
\end{align}

\newpage
\subsubsection{Entropia di Shannon}
\begin{align}
	& H^{(n)}(p_1, \ldots, p_n) = - \sum_{i=1}^{n} p_i \log(p_i) \tag{Entropia}
\end{align}
\begin{proposition}
	L'entropia ha queste proprietà:
	\begin{itemize}
		\item È una funzione simmetrica
		\item $H^{(n)}(1, 0, \ldots, 0) = 0$
		\item È coerente tra n diversi: $H^{(n)}(p_1=0,p_2, \ldots, p_n) = H^{(n-1)}(p_2, \ldots, p_n) $
		\item $H^{(n)}(p_1, \ldots, p_n) \leq H^{(n)}(\frac{1}{n}, \ldots, \frac{1}{n})$
		\item Data una probabilità su $n \times m$ oggetti, $\Omega = \{x_{11}, \ldots, x_{ij}, \ldots, x_{nm}\}$, $\mathbb{P}(\{x_{ij}\})=q_{ij}$, considerando gli eventi $A_i = \{x_{i,1}, \ldots, x_{i,m}\}$, $\mathbb{P}(A_i)=p_i$:
		\begin{equation}
			H^{(nm)}(q_{11}, \ldots, q_{ij}, \ldots, q_{nm}) = H^{(n)}(p_1, \ldots, p_n) + \sum_{i=1}^{n} p_i H^{(m)}\bigg(\frac{q_{i1}}{p_i}, \ldots, \frac{q_{im}}{p_i}\bigg)
		\end{equation}
	\end{itemize}
\end{proposition}

\begin{theorem}[Teorema di Shannon]
	Una funzione continua che soddisfa queste proprietà deve avere la forma:
	\begin{equation}
		cH^{(n)} \quad\quad c>0
	\end{equation}
\end{theorem}

\subsubsection{Densità di probabilità}
\begin{align} 
	& \mathbb{P}(A) = \int_{A} f(x)dx \quad\quad A \subseteq \Omega \tag{Probabilità di una densità}\\
	& \mathbb{P}(A \cup B) = \int_{A \cup B} f(x)dx = \int_{A} f(x)dx + \int_{B} f(x)dx = \mathbb{P}(A) + \mathbb{P}(B) \quad\quad A \cap B = \emptyset \tag{Somma di probabilità} \\
	& \mathbb{P}(\{t\}) = \int_{\{t\}} f(x)dx=0 \tag{Probabilità di un punto} \\
	& f(x) = \begin{cases}
		1 & 0 \leq x \leq 1 \\
		0 & \text{altrove}
	\end{cases} \tag{Densità uniforme} \\
	& \mathcal{X}: \mathbb{R} \to \{0,1\} \quad\quad \mathcal{X}_S(x) = \begin{cases}
		1 & x \in S \\
		0 & x \notin S
	\end{cases} \tag{Funzione indicatrice}
\end{align}

\subsection{Variabili aleatorie}
\subsubsection{Legge di una variabile aleatoria}

\begin{align}
	& \mathbb{P}_X(A) = \mathbb{P}(X \in A) = \sum_{x_i \in A}p_X(x_i) \tag{Variabile aleatoria discreta} \\
	& \mathbb{P}_X(A) = \mathbb{P}(X \in A) = \int_{A} f(x)dx \tag{Variabile aleatoria continua} \\
	& \mathbb{P}(X \in A) = \mathbb{P}(a \leq X \leq b) = \int_{a}^{b} f(x)dx \tag{Variabile aleatoria continua con segmento}
\end{align}

\subsubsection{Funzione di ripartizione e quantili}
\begin{align}
	& F_X : \mathbb{R} \to [0,1] \quad\quad F_X(x) = \mathbb{P}\{X \leq x\} \tag{Cumulative Distribution Function} \\
	& \mathbb{P}\{a < X \leq b\} = F(b) - F(a) \\
	& F_X(t) = \sum_{x_i \leq t} p(x_i) \tag{c.d.f. discreta} \\
	& \mathbb{P}\{X=x\} = F(x) - F_{\_}(x) \\
	& f(x) = \frac{dF(x)}{dx} \tag{c.d.f. continua}
\end{align}

\begin{proposition}
	Proprietà della c.d.f.:
	\begin{itemize}
		\item $F$ non è decrescente ($x < y \Rightarrow F(x) \leq F(y)$)
		\item $\lim_{x \to - \infty} F(x) = 0 \quad\quad \lim_{x \to + \infty}F(x)  = 1$
		\item $F$ è continua a destra ($\forall x \in \mathbb{R} \quad F(x_n) \to F(x)$ per ogni successione $x_n \to x \quad x_n \geq x$)
	\end{itemize}
\end{proposition}

\begin{align}
	& r_\beta = inf\{r \in \mathbb{R} : F(r) \geq \beta\} \quad\quad \beta \in (0,1) \tag{$\beta$-quantile} \\
	& F^{\leftarrow} : (0,1) \to \mathbb{R} \quad\quad F^{\leftarrow}(t)=inf\{r \in \mathbb{R} : F(r) \geq t\} \tag{Inversa generalizzata}
\end{align}
\begin{proposition}
	Proprietà dell'inversa generalizzata:
	\begin{itemize}
		\item Se $F$ è strettamente crescente $F^{\leftarrow}=F^{-1}$
		\item $F^{\leftarrow}$ è sempre non decrescente
		\item $F^\leftarrow(F(t)) \leq t \quad \forall t \in \mathbb{R}$
		\item $F(F^\leftarrow(t)) \geq t \quad \forall t \in \mathbb{R}$
		\item $F^\leftarrow(t) \leq s \Leftrightarrow F(s) \geq t$
	\end{itemize}
\end{proposition}

\subsubsection{Variabili aleatorie notevoli discrete}
\begin{align}
	& \mathbb{P}(X=h) = \binom{n}{h}p^h(1-p)^{n-h} \tag{Binomiale $B(n,p)$} \\
	& \mathbb{P}(X=h) = (1-p)^{h-1}p\quad\quad h \in \mathbb{N}_0 \tag{Geometriche $G(p)$} \\
	& \mathbb{P}(X = k) = \frac{\binom{h}{k} \binom{n-h}{r-k}}{\binom{n}{r}} \quad\quad k=0,\ldots, h \tag{Ipergeometriche $I(n,h,r)$} \\
	& \mathbb{P}(X=h)=e^{-\lambda}\frac{\lambda^h}{h!} \quad\quad h \in \mathbb{N} \tag{Poisson $P(\lambda)$}
\end{align}
\begin{equation}
	\sum_{k=0}^{h} \binom{h}{k}\binom{n-h}{r-k} = \binom{n}{r} \tag{Identità di Vandermonde} 
\end{equation}

\subsubsection{Variabili aleatorie notevoli con densità}
\begin{align}
	& f(t)=\begin{cases}
		\frac{1}{b-a} & a<t<b\\
		0 & \text{altrove}
	\end{cases} \quad\quad
	F(t) = \begin{cases}
		0 & t \leq a \\
		\frac{t}{b-a} & 0 < t \leq b \\
		1 & t >b
	\end{cases} \tag{Uniformi su intervalli} \\
	& f(t)=\begin{cases}
			\lambda e^{-\lambda t} & t>0\\
			0 & t \leq 0
		\end{cases} \quad\quad
		F(t) = \begin{cases}
			1 - e^{-\lambda t} & t>0\\
			0 & t\leq 0
		\end{cases} \tag{Esponenziali} \\
		& f(t)=\begin{cases}
			\alpha x_m^\alpha t^{-1-\alpha} & t>x_m\\
			0 & t \leq x_m
		\end{cases} \quad\quad
		F(t) = \begin{cases}
			1 & t< x_m \\
			1 - (\frac{x_m}{t})^\alpha & t\geq x_m
		\end{cases} \tag{Pareto} \\
		& \varphi(x) = \frac{1}{\sqrt{2 \pi}} e^{\frac{-x^2}{2}} \quad\quad
		\Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x}e^{\frac{-t^2}{2}} \tag{Gaussiane standard $N(0,1)$} \\
		&  f_Y(t) = \frac{1}{\sigma}f_X\bigg(\frac{t-m}{\sigma}\bigg) = \frac{1}{\sqrt{2 \pi}\sigma} e^{-\frac{(t-m)^2}{2 \sigma^2}} \tag{Gaussiane non standard $N(m, \sigma^2)$}\\
		& F_Y(t) = \mathbb{P}\{Y \leq t\} = \mathbb{P}\{\sigma X + m \leq t\} = \mathbb{P}\bigg(X \leq \frac{t-m}{\sigma}\bigg) = \Phi \bigg(\frac{t-m}{\sigma}\bigg) \tag{Gaussiane non standard 2}
\end{align}

\begin{proposition}
	Proprietà delle Gaussiane standard:
	\begin{align}
		& \mathbb{P}\{-t \leq Z \leq t\} = \Phi(t) - \Phi(-t) = 2\Phi(t)-1 \\
		& \Phi(0) = \mathbb{P}\{X \geq 0\} = \mathbb{P} \{X \leq 0\} = \frac{1}{2}
	\end{align}
\end{proposition}
\begin{definition}[Variabili Gaussiane Non Standard - ]
	Data $X$ una v.a. $N(0,1)$ e $Y$ una v.a. del tipo $Y = \sigma X + m$.

\end{definition}

\subsubsection{Trasformazioni di variabili con densità}
\begin{equation}
	Y:\Omega \to \mathbb{R} \quad \quad Y = h \circ X \quad\quad F_Y(y) = \mathbb{P}\{Y\leq y\} = \mathbb{P}\{h(X) \leq y\}
\end{equation}

\begin{proposition}[Cambio di variabile]
	Sia $h: A \to B$ biunivoca, differenziabile e con inversa differenziabile:
	\begin{equation}
		f_Y(y) = \begin{cases}
			f_X(h^{-1}(y)) \cdot \bigg\lvert \frac{dh^{-1}(y)}{dy} \bigg\rvert & y \in B\\
			0 & y \notin B
		\end{cases}
	\end{equation}
	Se $h$ è crescente:
	\begin{align}
		& F_Y(y)=\mathbb{P}(Y\leq y) = \mathbb{P}(h(X) \leq y) = \mathbb{P}(X \leq h^{-1}(y)) = F_X(h^{-1}(y))\\
		& f_Y(y)=f_X(h^{-1}(y))\cdot \frac{dh^{-1}(y)}{dy}
	\end{align}
	Se $h$ è decrescente:
	\begin{equation}
		\mathbb{P}(h(X) \leq y) = \mathbb{P}(X \leq h^{-1}(y)) = 1 - F_X(h^{-1}(y))
	\end{equation}
\end{proposition}

\subsubsection{Valore atteso, varianza e momenti}
\begin{definition}[Valore atteso]
	\begin{equation}
		\mathbb{E}[X] = \sum_i x_i p_X(x_i) \quad\quad \mathbb{E}[X] = \int_{-\infty}^{+\infty} tf_X(t)dt
	\end{equation}
\end{definition}

\begin{proposition}
	Sia $X$ discreta, la variabile $g(x)$ ammette valore atteso se $\sum_i \lvert g(x_i) \rvert p(x_i) < +\infty$. In quel caso vale:
	\begin{equation}
		\mathbb{E}[g(X)] = \sum_i g(x_i)p(x_i)
	\end{equation}
	Sia $X$ con densità, la variabile $g(x)$ ammette valore atteso se $\int_{-\infty}*{+\infty} \lvert g(x) \rvert f(x) dx < +\infty$. In quel caso vale:
	\begin{equation}
		\mathbb{E}[g(X)] = \int_{-\infty}*{+\infty} g(x)f(x) dx
	\end{equation}
\end{proposition}
\begin{proposition}
	Se $X$ ha valore atteso, valgono:
	\begin{itemize}
		\item $\forall a,b \in \mathbb{R} \quad \mathbb{E}[aX+b] = a\mathbb{E}[X]+b \quad \mathbb{E}[b]=b$
		\item $\lvert \mathbb{E}[X] \rvert \leq E[\lvert X \rvert]$
		\item $\mathbb{P}(X \geq 0) = 1 \Longrightarrow \mathbb{E}[X] \geq 0$
	\end{itemize}
\end{proposition}

\begin{definition}[Momento di ordine $n$]
	\begin{equation}
		\mathbb{E}[X^n] \quad\quad \mathbb{E}[\lvert X \rvert ^n] < +\infty
	\end{equation}
\end{definition}

\begin{definition}[Disuguaglianza di Jensen]
	\begin{equation}
		\mathbb{E}[\lvert X \rvert ^m] ^{\frac{1}{m}} \leq \mathbb{E}[\lvert X \rvert^n]^{\frac{1}{n}} \quad\quad 1 \leq m \leq n
	\end{equation}
\end{definition}

\begin{definition}[Disuguaglianza di Markov]
	\begin{equation}
		a\mathbb{P}\{X \geq a\}\leq \mathbb{E}[X] \quad\quad a>0
	\end{equation}
\end{definition}

\begin{definition}[Varianza]
	\begin{equation}
		Var(X) = \mathbb{E}[(X-\mathbb{E}[X])^2] = \mathbb{E}[X^2] - \mathbb{E}[X]^2
	\end{equation}
\end{definition}

\begin{definition}[Scarto quadratico medio]
	\begin{equation}
		\sigma(X) = \sqrt{Var(X)}
	\end{equation}
\end{definition}

\begin{definition}[Disuguaglianza di Chebyshev]
	\begin{equation}
		\mathbb{P}\{\lvert X -\mathbb{E}[X] \rvert > d\} \leq \frac{Var(X)}{d^2} \quad\quad d>0
	\end{equation}
\end{definition}
\subsubsection{Momenti di variabili aleatorie notevoli}
\begin{proposition}[Variabili di Bernoulli - $B(1,p)$]
	\begin{equation}
		\mathbb{E}[X^k] = p \quad\quad Var(X) = p-p^2 = p(1-p) \quad \quad k \geq 1
	\end{equation}
\end{proposition}
\begin{proposition}[Variabili Binomiali]
	\begin{equation}
		\mathbb{E}[X] = np \quad\quad Var(X) = np(1-p)
	\end{equation}
\end{proposition}

\begin{proposition}[Variabili di Poisson]
	\begin{align}
		& \mathbb{E}[X] = \sum_{h=0}^{+\infty}he^{-\lambda} \frac{\lambda^h}{h!}=\lambda\\
		& \mathbb{E}[X^2] = \lambda + \lambda^2 \\
		& Var(X) = \lambda
	\end{align}
\end{proposition}

\begin{proposition}[Variabili uniformi su intervalli finiti]
	\begin{equation}
		\mathbb{E}[X] = \int_{a}^{b} \frac{x}{b-a}dx = \frac{a^2+ab+b^2}{3} \quad\quad Var(X) = \frac{(b-a)^2}{12}
	\end{equation}
\end{proposition}

\begin{proposition}[Variabili Esponenziali]
	\begin{equation}
		\mathbb{E}[X^n] = \frac{n!}{\lambda^n} \quad\quad Var(X) =\frac{1}{\lambda^2}
	\end{equation}
\end{proposition}

\begin{proposition}[Variabili Gaussiane Standard]
	\begin{align}
		& \mathbb{E}[X^{2h+1}] = 0 \\
		& \mathbb{E}[X^{2h+2}] = (2h+1)\mathbb{E}[X^{2h}] \\
		& Var(X) = 1
	\end{align}
\end{proposition}
\begin{proposition}[Variabili Gaussiane]
	\begin{equation}
		\mathbb{E}[Y] = \mathbb{E}[\sigma X + m]\quad\quad Var(Y) = Var(\sigma X + m) = \sigma^2 Var(X)
	\end{equation}
\end{proposition}

\subsection{Distribuzioni multivariate}
\subsubsection{Variabili doppie}
\begin{equation}
	\Omega \ni \omega \mapsto (X(\omega), Y(\omega)) \in \mathbb{R}^2 \quad\quad \mathbb{P}_{(X,Y)}(A) = \mathbb{P}((X,Y) \in A) = \mathbb{P}\{\omega \in \Omega : (X(\omega), Y(\Omega)) \in A\}
\end{equation}

\begin{proposition}[Distribuzione di probabilità di variabile doppia discreta]
	\begin{equation}
		p(x_i,y_j) = \mathbb{P}(X=x_i, Y=y_j)
	\end{equation}
	\begin{equation}
		\mathbb{P}_{(X,Y)}(A) = \mathbb{P}\{(X,Y) \in A\} = \sum_{(x_i, y_j) \in A} p(x_i, y_j)
	\end{equation}
	\begin{align}
		& p_X(x_i) = \sum_{j=1}^{\infty}p(x_i, y_j) \\
		& p_Y(y_j) = \sum_{i=1}^{\infty}p(x_i, y_j)
	\end{align}
\end{proposition}

\begin{proposition}[Distribuzione di probabilità di variabile doppia con densità]
	\begin{equation}
		\mathbb{P}_{(X,Y)}(A) = \mathbb{P}\{(X,Y) \in A\} = \int\int_A f(x,y)dxdy
	\end{equation}
	\begin{align}
		& f_X(x) = \int_{-\infty}^{+\infty} f(x,y) dy \\
		& f_Y(y) = \int_{-\infty}^{+\infty} f(x,y) dx 
	\end{align}
\end{proposition}

\subsubsection{Indipendenza di variabili aleatorie}
\begin{definition}[Variabili aleatorie indipendenti]
	\begin{equation}
		\mathbb{P}(X_1 \in A_1, \ldots, X_n \in A_n) = \mathbb{P}(X_1 \in A_1) \cdot \ldots \cdot \mathbb{P}(X_n \in A_n)
	\end{equation}
	\begin{align}
		&p(x_i, y_j) = p_X(x_i) \cdot p_Y(y_j) \quad\quad \forall(x_i, y_j) \\
		& f(x,y) = f_X(x) \cdot f_Y(y) \quad\quad \forall (x,y)
	\end{align}
\end{definition}
\subsubsection{Funzioni di variabili indipendenti}
\begin{proposition}[Somma di Binomiali]
	\begin{equation}
		X \to B(n,p) \quad Y \to B(m,p) \Longrightarrow Z=X+Y \to B(n+m, p)
	\end{equation}
\end{proposition}

\begin{proposition}[Funzione di massa di somma di variabili discrete]
	\begin{equation}
		Z = X+Y \Longrightarrow p_Z(n) = \sum_{h=0}^{n}p_X(h) \cdot p_Y(n-h)
	\end{equation}
\end{proposition}

\begin{proposition}[Funzione di massa di somma di variabili con densità o formula della convoluzione]
	\begin{equation}
		Z = X+Y \Longrightarrow p_Z(n) = \int_{-\infty}^{+\infty} f_X(x)f_Y(z-x)dx = \int_{-\infty}^{+\infty} f_Y(y)f_X(z-y)dy
	\end{equation}
\end{proposition}

\subsubsection{Covarianza e correlazione}
\begin{proposition}[Valore atteso di somma di variabili]
	Dati $X$ e $Y$ con valore atteso:
	\begin{itemize}
		\item $\mathbb{E}[X+Y] = \mathbb{E}[X] + \mathbb{E}[Y]$ \\
		\item $X \geq Y \Longrightarrow \mathbb{E}[X] \geq \mathbb{E}[Y]$
	\end{itemize}
	Se sono anche indipendenti:
	\begin{align}
		& \mathbb{E}[XY] = \mathbb{E}[X] \cdot \mathbb{E}[Y] \\
		& \mathbb{E}[h(X)k(Y)] = \mathbb{E}[h(X)] \cdot \mathbb{E}[k(Y)] 
	\end{align}
\end{proposition}

\begin{proposition}[Disuguaglianza di Schwartz]
	\begin{equation}
		\mathbb{E}[\lvert XY \rvert] \leq \sqrt{\mathbb{E}[X^2]} \cdot \sqrt{\mathbb{E}[Y^2]}
	\end{equation}
\end{proposition}

\begin{definition}[Covarianza]
	\begin{equation}
		Cov(X,Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
	\end{equation}
\end{definition}

\begin{definition}[Coefficiente di correlazione]
	\begin{equation}
		\rhd(X,Y) = \frac{Cov(X,Y)}{\sigma(X)\sigma(Y)}
	\end{equation}
\end{definition}

\subsection{Variabili indipendenti e teoremi limite}
\begin{definition}[Convergenza in probabilità]
	\begin{equation}
		\forall \epsilon > 0 \quad\quad \lim_{n \to \infty} \mathbb{P}(\lvert X_n - X \rvert > \epsilon) = 0
	\end{equation}
\end{definition}
\begin{proposition}[Convergenza in probabilità ad una costante]
	\begin{equation}
		\lim_{n \to \infty} \mathbb{E}[X_n] = c \in \mathbb{R} \land \lim_{n \to \infty} Var(X_n)=0 \Longrightarrow (X_n)_{n\geq 1} \longrightarrow c
	\end{equation}
\end{proposition}

\begin{proposition}
	\begin{equation}
		\forall \epsilon > 0 \quad \lim_{n \to \infty} \mathbb{P}(\lvert S_n^2 - \sigma^2 \rvert > \epsilon) = 0
	\end{equation}
\end{proposition}

\begin{definition}[Convergenza in distribuzione]
	\begin{equation}
		\lim_{n \to \infty} F_n(t)=F(t) \quad \forall t \in \mathbb{R} \Longrightarrow (X_n)_{n \geq 1} \longrightarrow X 
	\end{equation}
\end{definition}

\begin{definition}[Teorema centrale del limite]
	\begin{equation}
		\lim_{n \to \infty}\mathbb{P}\bigg(a \leq \frac{X_1 + \ldots + X_n - n\mu}{\sigma \sqrt{n}}\bigg) = \frac{1}{\sqrt{2\pi}}\int_{a}^{b}e^{-1\frac{x^2}{2}}dx = \Phi(b)-\phi(a)
	\end{equation}
\end{definition}
\begin{definition}[Approssimazione a variabile Gaussiana]
	\begin{equation}
		\frac{X_1 + \ldots + X_n -n\mu}{\sigma \sqrt{n}} = \sqrt{n}\frac{\bar{X}_n-\mu}{\sigma} \quad\quad n\geq 50
	\end{equation}
\end{definition}

\subsubsection{Variabili Chi-Quadro e di Student}
\begin{definition}[Gamma di Eulero]
	\begin{equation}
		\Gamma(r)=\int_{0}^{+\infty} x^{r-1}e^{-x}dx = (r-1)\Gamma(r-1) \quad\quad r>0
	\end{equation}
\end{definition}
\begin{definition}[Densità Gamma - $\Gamma(r,\lambda)$]
	\begin{equation}
		f(x) = \begin{cases}
			\frac{1}{\Gamma(r)} \lambda^rx^{r-1}e^{-\lambda x} & x >0\\
			0& z \leq 0
		\end{cases}
	\end{equation}
	\begin{align}
		& \mathbb{E}[X^\beta] = \frac{\Gamma(r+\beta)}{\Gamma(r)\lambda^\beta} \\
		& Var(X)=\frac{r}{\lambda^2}
	\end{align}
\end{definition}
\begin{proposition}[Somma di densità gamma]
	\begin{equation}
		X\to \Gamma(r, \lambda) \quad Y \to \Gamma(s, \lambda) \Longrightarrow (X+Y) \to \Gamma(r+s, \lambda)
	\end{equation}
\end{proposition}

\begin{definition}[Densità Chi-quadro]
	\begin{equation}
		X_1, \ldots, X_n \to N(0,1) \Longrightarrow (X_1^2, \ldots, X_n^") \to \Gamma(\frac{n}{2}, \frac{1}{2}) = \mathcal{X}^2(n)
	\end{equation}
	\begin{equation}
		F_{X^2}(t) = \mathbb{P}(X^2 \leq t) = \begin{cases}
			0 & t<0 \\
			\mathbb{P}(-\sqrt{t} \leq X \leq \sqrt{t}) = 2F_X(\sqrt{t})-1 & t \geq 1
		\end{cases}
	\end{equation}
\end{definition}

\begin{proposition}[Approssimazioni Chi-quadro]
	Data $C_n$ variabile con densità $\mathcal{X}^2(n)$:
	\begin{itemize}
		\item  $\lim_{n \to \infty} \frac{C_2}{n} \approx 1$
		\item $\lim_{n \to \infty} \frac{C_n -n}{\sqrt{2n}} \approx N(0,1)$
	\end{itemize}
\end{proposition}

\begin{definition}[Densità Student]
	\begin{equation}
		T_n = \sqrt{n} \frac{X}{\sqrt{C_n}} \quad \quad X \to N(0,1) \quad C_n \to \mathcal{X}^2(n)
	\end{equation}
	\begin{equation}
		f_{T_n}(t) = \frac{\Gamma(\frac{n}{2} + \frac{1}{2})}{\sqrt{n\pi} \Gamma(\frac{n}{2})} \bigg(1+\frac{t^2}{n}\bigg)^{-\frac{n}{2}-\frac{1}{2}}
	\end{equation}
\end{definition}
\begin{proposition}
	\begin{equation}
		(T_n)_{n\geq 1} \longrightarrow N(0,1)
	\end{equation}
\end{proposition}
\begin{proposition}
	Date $X_1, \ldots, X_n$ v.a. $N(m, \sigma^2)$:
	\begin{itemize}
		\item $\bar{X}_n$ e $S^2_n$ sono indipendenti
		\item $\bar{X} \to N(m, \frac{\sigma^2}{n})$
		\item $\frac{n-1}{\sigma^2}S_n^2 \to \mathcal{X}^2(n-1)$
		\item $T=\sqrt{n}\frac{(\bar{X}_n - m)}{S} \to T(n-1)$
	\end{itemize}
\end{proposition}

\subsection{Campioni statistici e stimatori}
\subsubsection{Stima parametrica}
\begin{definition}[Stimatore corretto]
	Dato un parametro $\theta$:
	\begin{equation}
		\mathbb{E}_\theta[g(X_1, \ldots, X_n)] = \theta
	\end{equation}
\end{definition}
\begin{proposition}
	\begin{align}
		& \mathbb{E}[\bar{X}] = \mathbb{E}[X_i] \\
		& Var(\bar{X}) = \frac{Var(X_i)}{n} \\
		& \mathbb{E}[S^2_n] = Var(X_i)
	\end{align}
\end{proposition}
\begin{definition}[Stimatore consistente]
	\begin{equation}
		\lim_{n \to \infty} \mathbb{P}_\theta\{\lvert g_n(X-1, \ldots, X_n) - \theta \rvert > \epsilon\} = 0
	\end{equation}
\end{definition}
\begin{definition}[Stimatore efficiente]
	\begin{equation}
		Var_\theta(g(X_1, \ldots,X_n)) \leq Var_\theta(h(X_1, \ldots, X_n))
	\end{equation}
\end{definition}

\subsubsection{Massima verosimiglianza e metodo dei momenti}
\begin{definition}[Funzione di verosimiglianza]
	\begin{align}
		& L(\theta, x_1, \ldots, x_n) = \prod_{i=1}^{n} p_\theta(x_i) \\
		& L(\theta, x_1, \ldots, x_n) = \prod_{i=1}^{n} f_\theta(x_i)
	\end{align}
\end{definition}
\begin{definition}[Stima di massima verosimiglianza]
	\begin{equation}
		L(\hat{\theta}; x_1, \ldots, x_n) = \max_{\theta \in \Theta} L(\theta; x_1, \ldots, x_n)
	\end{equation}
\end{definition}
\begin{definition}[Metodo dei momenti]
	\begin{equation}
		\mathbb{E}_{\bar{\theta}}[X^k] = \frac{1}{n}\sum_{i=1}^{n}x_i^k \quad\quad \forall (x_1, \ldots, x_n)
	\end{equation}
\end{definition}

\subsection{Intervalli di fiducia}
\begin{definition}[Intervallo di fiducia per la media]
	\begin{align}
		& \bigg[\bar{X}_n \pm \frac{\sigma}{\sqrt{n}}q_{1-\frac{\alpha}{2}}\bigg]\\
		& S_n^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2 \\
		& \bigg[\bar{X}_n \pm \frac{S_n}{\sqrt{n}}q_{1-\frac{\alpha}{2}}\bigg]
	\end{align}
\end{definition}
\begin{definition}[Intervallo di fiducia per la media - Bernoulli]
	\begin{equation}
		\bigg[\bar{X}_n \pm \sqrt{\frac{\bar{X}_n(1-\bar{X}_n)}{n}} q_{1-\frac{\alpha}{2}}\bigg]
	\end{equation}
\end{definition}
\begin{definition}[Intervallo di fiducia per la varianza - Gaussiani]
	\begin{equation}
		\bigg( 0,\frac{(n-1)S^2_n}{\mathcal{X}^2_{\alpha, n-1}} \bigg] \quad\quad \bigg[ \frac{(n-1)S^2_n}{\mathcal{X}^2_{1-\alpha, n-1}}, +\infty \bigg)
	\end{equation}
\end{definition}

\subsection{Test statistici}
\begin{definition}[Regione critica di livello $\alpha$]\newcommand{\eqname}[1]{\tag*{#1}}%
	\begin{equation}
		C = \bigg\{\sqrt{n}\frac{\lvert \bar{X} - m_0 \rvert}{\sigma} > q_{1-\frac{\alpha}{2}}\bigg\} = \bigg\{\lvert \bar{X}_n - m_0 \rvert > \frac{\sigma}{\sqrt{n}}q_{1-\frac{\alpha}{2}}\bigg\}
	\end{equation}
\end{definition}
\begin{definition}[p-value]
	\begin{equation}
		\bar{\alpha} = \mathbb{P}_{m_0}\bigg( \sqrt{n}\frac{\bar{X}_n - m_0 \rvert}{\sigma} > \frac{\sqrt{n}}{\sigma}\lvert\bar{X}_n - m_0 \rvert\bigg) = 2 \bigg[1- \Phi\bigg(\frac{\sqrt{n}}{\sigma}\lvert\bar{X}_n - m_0 \rvert\bigg)\bigg]
	\end{equation}
\end{definition}
\begin{definition}[Curva operativa]
	\begin{equation}
		\beta(m)= \Phi\bigg(\sqrt{n}\frac{\lvert m_0 - m \rvert}{\sigma}+q_{1-\alpha}\bigg)
	\end{equation}
\end{definition}
\subsubsection{Campione di Bernoulli}
\begin{equation}
	C = \bigg\{\frac{\sqrt{n}\lvert \bar{X} - m_0 \rvert}{\sqrt{p_0(1-p_0)}} > q_{1-\frac{\alpha}{2}}\bigg\} 
\end{equation}
\begin{equation}
	\bar{\alpha} =  2 \bigg[1- \Phi\bigg(\frac{\sqrt{n} \lvert\bar{X}_n - p_0 \rvert}{\sqrt{p_o(1-p_0)}}\bigg)\bigg]
\end{equation}
\subsubsection{Campione Gaussiano}
\begin{equation}
	C = \bigg\{\frac{n-1}{\sigma_0^2} S_n^2 > \mathcal{X}^2_{2-\alpha,n-1}\bigg\} 
\end{equation}
\begin{equation}
	\bar{\alpha} =  1-G_{n-1}\bigg(\frac{\sum_{i=1}^{n} (x_i-\bar{x}_n)^2}{\sigma_0^2}\bigg)
\end{equation}
\begin{equation}
	\beta(\sigma)= G_{n-1}\bigg(\frac{\sigma_0^2}{\sigma^2}\mathcal{X}^2_{1-\alpha, n-1}\bigg)
\end{equation}